From 3760960a9d7f23d7e0669f47254ddf2a9576da6f Mon Sep 17 00:00:00 2001
From: Joey Riches <josephriches@gmail.com>
Date: Sun, 26 Dec 2021 14:57:22 +0000
Subject: [PATCH 1/5] fetcher: replace urlgrabber dependency with urllib2

urllib2 is part of the python standard library meaning we can dropkick
urlgrabber from the repo and reduces the moving parts that could potentially
cause headaches.

There was already some urllib2 usage w.r.t partial downloads, however,
we now rely on urllib2 exclusively.

This commit was originally taken from PiSiLinux/PiSi and rewritten for urllib2 usage
as opposed to urllib3.
---
 pisi/fetcher.py | 177 ++++++++++++++++++++++++------------------------
 1 file changed, 87 insertions(+), 90 deletions(-)

diff --git a/pisi/fetcher.py b/pisi/fetcher.py
index 8a89142..7636d1a 100644
--- a/pisi/fetcher.py
+++ b/pisi/fetcher.py
@@ -16,10 +16,12 @@ all kinds of things: source tarballs, index files, packages, and God
 knows what."""
 
 # python standard library modules
-import os
-import time
 import base64
+import contextlib
+import os
 import shutil
+import time
+import urllib2
 
 import gettext
 __trans = gettext.translation('pisi', fallback=True)
@@ -36,58 +38,55 @@ class FetchError(pisi.Error):
     pass
 
 
-class UIHandler:
-    def __init__(self, progress):
-        self.filename        = None
-        self.url             = None
-        self.basename        = None
-        self.downloaded_size = 0
-        self.percent         = None
-        self.rate            = 0.0
-        self.size            = 0
-        self.eta             = '--:--:--'
-        self.symbol          = '--/-'
-        self.last_updated    = 0
-        self.exist_size      = 0
-
-    def start(self, archive, url, basename, size, text):
+class FetchHandler:
+    def __init__(self, url, archive, bandwidth_limit):
+        self.url                = url
+        self.percent            = None
+        self.rate               = 0.0
+        self.size               = 0
+        self.eta                = '--:--:--'
+        self.symbol             = '--/-'
+        self.last_updated       = 0
+        self.filename           = url.filename()
+        self.total_size         = 0.0
+        self.exist_size         = 0
+        self.bandwidth_limit    = bandwidth_limit
         if os.path.exists(archive):
             self.exist_size = os.path.getsize(archive)
-        self.filename   = util.remove_suffix(ctx.const.partial_suffix, basename)
-        self.url        = url
-        self.basename   = basename
-        self.total_size = size or 0
-        self.text       = text
 
-        self.now    = lambda: time.time()
+        self.now = lambda: time.time()
         self.t_diff = lambda: self.now() - self.s_time
 
         self.s_time = self.now()
 
-    def update(self, size):
-
-        if self.size == size:
-            return
-
-        self.size = size
+    def update(self, blocknum, bs, size):
+        self.total_size = size + self.exist_size
+        self.size = blocknum * bs + self.exist_size
         if self.total_size:
-            self.percent = (size * 100.0) / self.total_size
+            self.percent = self.size * 100.0 / self.total_size
+            if self.percent > 100:
+                self.percent = 100
         else:
             self.percent = 0
 
-        if int(self.now()) != int(self.last_updated) and size > 0:
+        if int(self.now()) != int(self.last_updated) and self.size > 0:
             try:
-                self.rate, self.symbol = util.human_readable_rate((size - self.exist_size) / (self.now() - self.s_time))
+                self.rate, self.symbol = util.human_readable_rate(self.size / (self.now() - self.s_time))
             except ZeroDivisionError:
                 return
             if self.total_size:
-                self.eta  = '%02d:%02d:%02d' %\
+                self.eta = '%02d:%02d:%02d' %\
                     tuple([i for i in time.gmtime((self.t_diff() * (100 - self.percent)) / self.percent)[3:6]])
 
         self._update_ui()
+        self._limit_bandwidth()
 
-    def end(self, read):
-        pass
+    def _limit_bandwidth(self):
+        if self.bandwidth_limit:
+            expected_time = (self.size - self.exist_size) / self.bandwidth_limit
+            sleep_time = expected_time - self.t_diff()
+            if sleep_time > 0:
+                time.sleep(sleep_time)
 
     def _update_ui(self):
         ctx.ui.display_progress(operation       = "fetching",
@@ -122,54 +121,59 @@ class Fetcher:
 
         util.ensure_dirs(self.destdir)
 
-    def test(self, timeout=3):
-        import urlgrabber
-
-        try:
-            urlgrabber.urlopen(self.url.get_uri(),
-                           http_headers = self._get_http_headers(),
-                           ftp_headers  = self._get_ftp_headers(),
-                           proxies      = self._get_proxies(),
-                           retry = 3,
-                           timeout      = timeout,
-                           user_agent   = 'eopkg Fetcher/' + pisi.__version__)
-        except urlgrabber.grabber.URLGrabError:
-            return False
-
-        return True
-
-    def fetch (self):
+    def fetch(self):
         """Return value: Fetched file's full path.."""
 
-        # import urlgrabber module
-        try:
-            import urlgrabber
-        except ImportError:
-            raise FetchError(_('Urlgrabber needs to be installed to run this command'))
-
         if not self.url.filename():
             raise FetchError(_('Filename error'))
 
         if not os.access(self.destdir, os.W_OK):
-            raise FetchError(_('Access denied to write to destination directory: "%s"') % (self.destdir))
+            raise FetchError(_('Access denied to write to destination directory: "%s"') % self.destdir)
 
         if os.path.exists(self.archive_file) and not os.access(self.archive_file, os.W_OK):
-            raise FetchError(_('Access denied to destination file: "%s"') % (self.archive_file))
+            raise FetchError(_('Access denied to destination file: "%s"') % self.archive_file)
 
         try:
-            urlgrabber.urlgrab(self.url.get_uri(),
-                           self.partial_file,
-                           progress_obj = UIHandler(self.progress),
-                           http_headers = self._get_http_headers(),
-                           ftp_headers  = self._get_ftp_headers(),
-                           proxies      = self._get_proxies(),
-                           throttle     = self._get_bandwith_limit(),
-                           reget        = self._test_range_support(),
-                           copy_local   = 1,
-                           retry = 3, # retry 3 times
-                           timeout = 120, # Reduce from default of 5 minutes to 2 minutes
-                           user_agent   = 'eopkg Fetcher/' + pisi.__version__)
-        except urlgrabber.grabber.URLGrabError, e:
+            fetch_handler = FetchHandler(self.url, self.partial_file, self._get_bandwidth_limit())
+
+            proxy = urllib2.ProxyHandler(self._get_proxies())
+            opener = urllib2.build_opener(proxy)
+            opener.addheaders = self._get_headers()
+            urllib2.install_opener(opener)
+            has_range_support = self._test_range_support()
+
+            if has_range_support and os.path.exists(self.partial_file):
+                partial_file_size = os.path.getsize(self.partial_file)
+                opener.addheaders.append(('Range', 'bytes=%s-' % partial_file_size))
+
+            with contextlib.closing(urllib2.urlopen(self.url.get_uri(), timeout = 120)) as fp:
+                headers = fp.info()
+
+                if self.url.is_local_file():
+                    return os.path.normpath(self.url.path())
+
+                if has_range_support:
+                    tfp = open(self.partial_file, 'ab')
+                else:
+                    tfp = open(self.partial_file, 'wb')
+
+                with tfp:
+                    bs = 1024 * 8
+                    size = -1
+                    read = 0
+                    blocknum = 0
+                    if "content-length" in headers:
+                        size = int(headers["Content-Length"])
+                    fetch_handler.update(blocknum, bs, size)
+                    while True:
+                        block = fp.read(bs)
+                        if not block:
+                            break
+                        read += len(block)
+                        tfp.write(block)
+                        blocknum += 1
+                        fetch_handler.update(blocknum, bs, size)
+        except urllib2.URLError as e:
             raise FetchError(_('Could not fetch destination file "%s": %s') % (self.url.get_uri(), e))
 
         if os.stat(self.partial_file).st_size == 0:
@@ -180,19 +184,13 @@ class Fetcher:
 
         return self.archive_file
 
-    def _get_http_headers(self):
+    def _get_headers(self):
         headers = []
-        if self.url.auth_info() and (self.url.scheme() == "http" or self.url.scheme() == "https"):
+        if self.url.auth_info():
             enc = base64.encodestring('%s:%s' % self.url.auth_info())
-            headers.append(('Authorization', 'Basic %s' % enc),)
-        return tuple(headers)
-
-    def _get_ftp_headers(self):
-        headers = []
-        if self.url.auth_info() and self.url.scheme() == "ftp":
-            enc = base64.encodestring('%s:%s' % self.url.auth_info())
-            headers.append(('Authorization', 'Basic %s' % enc),)
-        return tuple(headers)
+            headers.append(('Authorization', 'Basic %s' % enc))
+        headers.append(('User-Agent', 'eopkg Fetcher/' + pisi.__version__))
+        return headers
 
     def _get_proxies(self):
         proxies = {}
@@ -211,7 +209,7 @@ class Fetcher:
 
         return proxies
 
-    def _get_bandwith_limit(self):
+    def _get_bandwidth_limit(self):
         bandwidth_limit = ctx.config.options.bandwidth_limit or ctx.config.values.general.bandwidth_limit
         if bandwidth_limit and bandwidth_limit != "0":
             ctx.ui.warning(_("Bandwidth usage is limited to %s KB/s") % bandwidth_limit)
@@ -221,24 +219,23 @@ class Fetcher:
 
     def _test_range_support(self):
         if not os.path.exists(self.partial_file):
-            return None
+            return False
 
-        import urllib2
         try:
             file_obj = urllib2.urlopen(urllib2.Request(self.url.get_uri()))
         except urllib2.URLError:
             ctx.ui.debug(_("Remote file can not be reached. Previously downloaded part of the file will be removed."))
             os.remove(self.partial_file)
-            return None
+            return False
 
         headers = file_obj.info()
         file_obj.close()
         if headers.has_key('Content-Length'):
-            return 'simple'
+            return True
         else:
             ctx.ui.debug(_("Server doesn't support partial downloads. Previously downloaded part of the file will be over-written."))
             os.remove(self.partial_file)
-            return None
+            return False
 
 
 # helper function
-- 
2.35.1

